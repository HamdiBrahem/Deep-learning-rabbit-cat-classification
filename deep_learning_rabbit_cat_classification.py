# -*- coding: utf-8 -*-
"""Deep learning rabbit cat classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gk5lxSJY-o4dq2nS_ZsW4iQPak45VSjr
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.preprocessing import LabelBinarizer

train_dir = '/content/drive/My Drive/Dataset 1/train-cat-rabbit'
val_dir = '/content/drive/My Drive/Dataset 1/val-cat-rabbit'

# Création de l'instance ImageDataGenerator avec normalisation et augmentation des données
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

val_datagen = ImageDataGenerator(rescale=1./255)

# Charger les images de l'ensemble d'entraînement
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(300, 300),
    batch_size=32,
    class_mode='binary'
)

# Charger les images de l'ensemble de validation
val_generator = val_datagen.flow_from_directory(
    val_dir,
    target_size=(300, 300),
    batch_size=32,
    class_mode='binary'
)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization

# Définir le modèle CNN
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(300, 300, 3)),
    BatchNormalization(),
    MaxPooling2D((2, 2)),

    Conv2D(64, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),

    Conv2D(128, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),

    Conv2D(256, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),

    Conv2D(512, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),

    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

# Compiler le modèle
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Afficher le résumé du modèle
model.summary()

# Entraîner le modèle
history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // train_generator.batch_size,
    validation_data=val_generator,
    validation_steps=val_generator.samples // val_generator.batch_size,
    epochs=20
)

import numpy as np
from sklearn.metrics import classification_report, confusion_matrix

# Évaluer le modèle sur l'ensemble de validation
val_loss, val_accuracy = model.evaluate(val_generator, steps=val_generator.samples // val_generator.batch_size)
print(f'Validation Accuracy: {val_accuracy:.2f}')

# Prédictions sur l'ensemble de validation
val_generator.reset()
predictions = model.predict(val_generator, steps=(val_generator.samples // val_generator.batch_size) + 1, verbose=1)
predicted_classes = np.where(predictions > 0.5, 1, 0).flatten()

# Vraies étiquettes
true_classes = val_generator.classes

# Ajustement pour correspondre au nombre de vraies étiquettes
if len(predicted_classes) > len(true_classes):
    predicted_classes = predicted_classes[:len(true_classes)]

# Vérification des longueurs pour débogage
print(f'Number of true classes: {len(true_classes)}')
print(f'Number of predicted classes: {len(predicted_classes)}')

# Vérification de la taille des vraies étiquettes et des prédictions
assert len(predicted_classes) == len(true_classes), "Mismatch in the number of predicted and true classes"

# Classes des labels
class_labels = list(val_generator.class_indices.keys())

# Rapport de classification
report = classification_report(true_classes, predicted_classes, target_names=class_labels)
print(report)

# Matrice de confusion
conf_matrix = confusion_matrix(true_classes, predicted_classes)
print(conf_matrix)

# Importer les bibliothèques nécessaires
import matplotlib.pyplot as plt

# Extraire les données de l'historique d'entraînement
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

# Définir le nombre d'époques
epochs = range(1, len(acc) + 1)

# Créer une figure pour les courbes
plt.figure(figsize=(12, 6))

# Tracer la courbe d'accuracy
plt.subplot(1, 2, 1)
plt.plot(epochs, acc, 'b', label='Training accuracy')
plt.plot(epochs, val_acc, 'r', label='Validation accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Tracer la courbe de loss
plt.subplot(1, 2, 2)
plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Afficher les courbes
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Afficher quelques exemples d'images prédites avec leurs étiquettes réelles
def plot_predictions(generator, model, class_labels, num_images=10):
    generator.reset()
    for i in range(num_images):
        x, y = generator.next()
        img = x[0]
        true_label = y[0]
        prediction = model.predict(np.expand_dims(img, axis=0))
        predicted_label = np.where(prediction > 0.5, 1, 0)[0][0]

        plt.figure(figsize=(3, 3))
        plt.imshow(img)
        plt.title(f'True: {class_labels[int(true_label)]}, Pred: {class_labels[int(predicted_label)]}')
        plt.axis('off')
        plt.show()

# Afficher les prédictions
plot_predictions(val_generator, model, class_labels, num_images=5)